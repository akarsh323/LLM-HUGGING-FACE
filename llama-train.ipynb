{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":6703755,"sourceType":"datasetVersion","datasetId":3863727},{"sourceId":9099380,"sourceType":"datasetVersion","datasetId":5480739},{"sourceId":9113612,"sourceType":"datasetVersion","datasetId":5500159},{"sourceId":81878,"sourceType":"modelInstanceVersion","modelInstanceId":68806,"modelId":91102},{"sourceId":81881,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install libs\n!pip install -qq peft\n!pip install -qq bitsandbytes\n!pip install -qq accelerate\n!pip install --upgrade transformers\n!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q \n!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD\n!pip show transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-22T09:43:35.287931Z","iopub.execute_input":"2024-08-22T09:43:35.288192Z","iopub.status.idle":"2024-08-22T09:45:33.030868Z","shell.execute_reply.started":"2024-08-22T09:43:35.288166Z","shell.execute_reply":"2024-08-22T09:45:33.029832Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.44.0)\nCollecting transformers\n  Downloading transformers-4.44.1-py3-none-any.whl (9.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.66.5)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2024.7.24)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.15.4)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (24.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.0\n    Uninstalling transformers-4.44.0:\n      Successfully uninstalled transformers-4.44.0\nSuccessfully installed transformers-4.44.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.19.0 requires torch==2.4.0, but you have torch 2.1.2+cpu which is incompatible.\ntorchaudio 2.4.0 requires torch==2.4.0, but you have torch 2.1.2+cpu which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mName: transformers\nVersion: 4.44.1\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\nHome-page: https://github.com/huggingface/transformers\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\nAuthor-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /usr/local/lib/python3.10/site-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\nRequired-by: peft\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip show transformers","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:45:33.032652Z","iopub.execute_input":"2024-08-22T09:45:33.032957Z","iopub.status.idle":"2024-08-22T09:45:36.285216Z","shell.execute_reply.started":"2024-08-22T09:45:33.032924Z","shell.execute_reply":"2024-08-22T09:45:36.284421Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Name: transformers\nVersion: 4.44.1\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\nHome-page: https://github.com/huggingface/transformers\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\nAuthor-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /usr/local/lib/python3.10/site-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\nRequired-by: peft\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gc\nimport re\nfrom time import time\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport torch\nimport transformers\nfrom sklearn.metrics import accuracy_score\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nimport torch.nn.functional as F\n\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\nfrom spmd_util import partition_module\n\ntqdm.pandas()\n\nprint(f'Torch Version: {torch.__version__}')","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:45:36.286430Z","iopub.execute_input":"2024-08-22T09:45:36.286718Z","iopub.status.idle":"2024-08-22T09:45:47.795978Z","shell.execute_reply.started":"2024-08-22T09:45:36.286687Z","shell.execute_reply":"2024-08-22T09:45:47.794846Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"name":"stdout","text":"Torch Version: 2.1.2+cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"class CFG:\n    NUM_EPOCHS = 1\n    BATCH_SIZE = 16\n    DROPOUT = 0.05 \n    MODEL_NAME = '/kaggle/input/llama-3.1/transformers/8b-instruct/1'\n    WEIGHTS_PATH = '/kaggle/input/llama-train-output/llama_3_finetuned_model_25000.pth'\n    SEED = 1024 \n    MAX_LENGTH = 1024 \n    NUM_WARMUP_STEPS = 128\n    LR_MAX = 5e-5 \n    NUM_LABELS = 3 \n    LORA_RANK = 4\n    LORA_ALPHA = 8\n    LORA_MODULES = ['o_proj', 'v_proj']\n    \nDEVICE = xm.xla_device() # Initialize TPU Device\nDEVICE","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:45:47.798951Z","iopub.execute_input":"2024-08-22T09:45:47.799663Z","iopub.status.idle":"2024-08-22T09:45:47.808130Z","shell.execute_reply.started":"2024-08-22T09:45:47.799631Z","shell.execute_reply":"2024-08-22T09:45:47.807323Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='xla', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"def set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n    # Set seed for all TPU cores\n    xm.set_rng_state(seed, device=xm.xla_device())  \n\nset_seeds(seed=CFG.SEED)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:45:47.809404Z","iopub.execute_input":"2024-08-22T09:45:47.809774Z","iopub.status.idle":"2024-08-22T09:45:47.827518Z","shell.execute_reply.started":"2024-08-22T09:45:47.809733Z","shell.execute_reply":"2024-08-22T09:45:47.826705Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True\n\n# save tokenizer to load offline during inference\ntokenizer.save_pretrained('tokenizer')","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:45:47.828596Z","iopub.execute_input":"2024-08-22T09:45:47.828986Z","iopub.status.idle":"2024-08-22T09:45:48.500613Z","shell.execute_reply.started":"2024-08-22T09:45:47.828941Z","shell.execute_reply":"2024-08-22T09:45:48.499598Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('tokenizer/tokenizer_config.json',\n 'tokenizer/special_tokens_map.json',\n 'tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# Utility function giving token length\ndef get_token_lengths(texts):\n    # tokenize and receive input_ids for reach text\n    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n    # return length of inputs_ids for each text\n    return [len(t) for t in input_ids]","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:45:48.501957Z","iopub.execute_input":"2024-08-22T09:45:48.502407Z","iopub.status.idle":"2024-08-22T09:45:48.507282Z","shell.execute_reply.started":"2024-08-22T09:45:48.502352Z","shell.execute_reply":"2024-08-22T09:45:48.506414Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/chatbot-arena/Data/train.csv\")\n# train = train.iloc[:25000, :]\n# train = train.iloc[25000:, :]\n\n\ndef process(input_str, str_type):\n    list_str = eval(input_str, {\"null\": \"\"})\n    list_str_processed = [\n        f\"<<{str_type} {i}>>: {text}\" + \"\\n\" + \"-\" * 10\n        for i, text in enumerate(list_str)\n        if text != \"\"\n    ]\n    return \"\\n\".join(list_str_processed)\n\n\ntrain.loc[:, \"prompt\"] = train[\"prompt\"].apply(lambda x: process(x, \"prompt\"))\ntrain.loc[:, \"response_a\"] = train[\"response_a\"].apply(\n    lambda x: process(x, \"Model A's response to prompt\")\n)\ntrain.loc[:, \"response_b\"] = train[\"response_b\"].apply(\n    lambda x: process(x, \"Model B's response to prompt\")\n)\n\n# Drop 'Null' for training\nindexes = train[(train.response_a == \"\") & (train.response_b == \"\")].index\ntrain.drop(indexes, inplace=True)\ntrain.reset_index(inplace=True, drop=True)\n\nprint(f\"Total {len(indexes)} Null response rows dropped\")\nprint(\"Total train samples: \", len(train))","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:45:48.508602Z","iopub.execute_input":"2024-08-22T09:45:48.509003Z","iopub.status.idle":"2024-08-22T09:45:55.504843Z","shell.execute_reply.started":"2024-08-22T09:45:48.508957Z","shell.execute_reply":"2024-08-22T09:45:55.504064Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Total 19 Null response rows dropped\nTotal train samples:  57458\n","output_type":"stream"}]},{"cell_type":"code","source":"train[\"text\"] = (\n    \"INSTRUCTION: \"\n    \"Below are user's prompts and responses from Model A and Model B for the corresponding prompts. \"\n    \"Which model grenerate the better responses for the prompts? \"\n    \"Be carefull about Hallucinations, long answer with explaination in details do not means that response is better. \"\n    \"The answer must be in one of three form: Model A, Model B, or Tie.\\n\"\n    + \"=\" * 50\n    + \"\\n\\nUser prompt: \"\n    + train[\"prompt\"]\n    + \"\\n\"\n    + \"=\" * 50\n    + \"\\n\\nModel A :\\n\"\n    + train[\"response_a\"]\n    + \"\\n\"\n    + \"=\" * 50\n    + \"\\n\\nModel B:\\n\"\n    + train[\"response_b\"]\n)\nprint(train[\"text\"][24993])","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:45:55.505804Z","iopub.execute_input":"2024-08-22T09:45:55.506093Z","iopub.status.idle":"2024-08-22T09:45:56.130170Z","shell.execute_reply.started":"2024-08-22T09:45:55.506055Z","shell.execute_reply":"2024-08-22T09:45:56.129479Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"INSTRUCTION: Below are user's prompts and responses from Model A and Model B for the corresponding prompts. Which model grenerate the better responses for the prompts? Be carefull about Hallucinations, long answer with explaination in details do not means that response is better. The answer must be in one of three form: Model A, Model B, or Tie.\n==================================================\n\nUser prompt: <<prompt 0>>: Who did listen to lana del rey in 2012\n----------\n<<prompt 1>>: What were likely those people listening in 2010\n----------\n<<prompt 2>>: What did fans of indie pop, alternative pop, and dream pop listen to in 2010\n----------\n<<prompt 3>>: What were fashionable to wear among such fans\n----------\n<<prompt 4>>: Did baroque pop exist in 2010\n----------\n<<prompt 5>>: What baroque pop were popular in late 60 early 70\n----------\n==================================================\n\nModel A :\n<<Model A's response to prompt 0>>: In 2012, Lana Del Rey gained significant popularity with the release of her second studio album, \"Born to Die.\" At that time, her music appealed to a wide range of listeners who enjoyed her dreamy, melancholic, and nostalgic musical style that blended elements of pop, alternative, and hip hop. Her fan base included millennials, indie music enthusiasts, alternative pop fans, and those drawn to her unique vocal style and cinematic sound. Notably, her music was also popular among artists, celebrities, and critics in the entertainment industry. Some notable names who praised or admitted to enjoying her music around that time include fellow musicians like Elton John, Courtney Love, and Azealia Banks, as well as celebrities like Lady Gaga, Amy Winehouse, and fashion designer Karl Lagerfeld. However, it's essential to remember that Lana Del Rey's music appealed to an extensive and diverse audience during this period.\n----------\n<<Model A's response to prompt 1>>: In 2010, various music genres and artists were popular. Some of the most prominent artists and genres from that year include:\n\n1. Pop: Artists like Katy Perry (with her hit \"California Gurls\"), Lady Gaga (with \"Bad Romance\"), and Ke$ha (known for \"Tik Tok\") were dominating the pop charts.\n\n2. Hip-hop and R&B: Eminem's album \"Recovery\" was a massive hit, while artists like Drake (with \"Over\") and Keri Hilson (with \"Pretty Girl Rock\") also enjoyed success.\n\n3. Rock: Kings of Leon's \"Sex on Fire\" and Muse's \"Resistance\" were popular among rock fans in 2010.\n\n4. Country: Carrie Underwood's \"Cowboy Casanova,\" Taylor Swift's \"Mine,\" and Lady Antebellum's \"Need You Now\" were among the top country songs.\n\n5. Electronic Dance Music (EDM): The electronic scene was on the rise, with artists like Swedish House Mafia and David Guetta gaining popularity.\n\nListeners in 2010 were likely enjoying music from these artists and genres, although individual preferences could vary widely based on geography, personal taste, and cultural background.\n----------\n<<Model A's response to prompt 2>>: In 2010, indie pop, alternative pop, and dream pop were not as mainstream as they would become in later years. However, there were still notable artists and albums that fans of these genres enjoyed. Some popular artists and albums from 2010 include:\n\n1. Indie Pop:\n   - Arcade Fire's \"The Suburbs\"\n   - Vampire Weekend's \"Contra\"\n   - MGMT's \"Congratulations\"\n\n2. Alternative Pop:\n   - Björk's \"Biophilia\"\n   - Bat for Lashes' \"Two Suns\"\n   - Miike Snow's \"Miike Snow\"\n\n3. Dream Pop:\n   - Beach House's \"Teen Dream\"\n   - Cocteau Twins' \"Treasure\" (though this album was originally released in 1984, it has continued to be popular among dream pop fans)\n   - Slowdive's \"Souvlaki\" (originally released in 1993, but also remained popular within the genre)\n\nKeep in mind that fans of these genres likely also explored lesser-known artists and underground acts to satisfy their musical preferences.\n----------\n<<Model A's response to prompt 3>>: In 2010, the fashion sense among fans of indie pop, alternative pop, and dream pop often reflected an eclectic and unique style. They might have been inspired by bohemian, vintage, or grunge influences. Some popular clothing items and trends among this demographic included:\n\n1. Vintage and thrifted clothes: Thrift shopping was a popular way to find unique pieces and support sustainability.\n\n2. Skinny jeans: Both for men and women, skinny jeans were a staple in many wardrobes.\n\n3. Vintage band tees: Wearing vintage band shirts from one's favorite indie, alternative, or dream pop artists was a common way to show support and express individuality.\n\n4. Plaid and flannel: Inspired by the grunge scene, plaid and flannel were fashionable and could be found in the form of shirts, skirts, or dresses.\n\n5. Lace and layers: Lace dresses, blouses, and shawls were popular. Mixing different textures and layering clothes was also a common practice.\n\n6. Unique accessories: Fans of these genres often favored unique and handmade accessories, such as statement necklaces, bracelets, or earrings.\n\n7. Converse and Dr. Martens: Comfortable and timeless, both Converse sneakers and Dr. Martens boots were fashionable among indie and alternative music fans.\n\nIt's essential to remember that fashion is subjective, and individual tastes varied greatly within this demographic. Many fans combined these fashion elements with their personal style preferences.\n----------\n<<Model A's response to prompt 4>>: Yes, the baroque pop genre existed in 2010. Although it originally gained popularity in the late 1960s, the genre continued to influence artists throughout the years. In 2010, some notable albums that could be classified as baroque pop include \"The Suburbs\" by Arcade Fire, \"The Age of Adz\" by Sufjan Stevens, and \"Teen Dream\" by Beach House. However, baroque pop is often used to describe a specific aesthetic or musical approach rather than a strict genre, allowing for its application to various musical styles in different time periods.\n----------\n<<Model A's response to prompt 5>>: Baroque pop, also known as chamber pop, became popular in the late 1960s and early 1970s. During this time, several artists and albums incorporated baroque influences into their music, often characterized by elaborate arrangements, orchestration, and a rich, dramatic sound. Here are some popular examples:\n\n1. The Left Banke - Their 1966 debut single \"Walk Away Renee\" is considered a classic example of baroque pop.\n\n2. The Zombies - Their 1968 album \"Odessey and Oracle\" features baroque pop elements, with songs like \"Time of the Season\" standing out.\n\n3. The Beach Boys - Their 1966 album \"Pet Sounds\" is considered an essential work in baroque pop, with its intricate arrangements and orchestration.\n\n4. The Beatles - Their late 1960s work, particularly \"Sgt. Pepper's Lonely Hearts Club Band\" (1967) and \"Abbey Road\" (1969), showcases baroque pop influences.\n\n5. Simon & Garfunkel - Their 1968 album \"Bookends\" includes baroque elements, as well as songs like \"Scarborough Fair\\/Canticle\" from their 1966 album \"Parsley, Sage, Rosemary and Thyme.\"\n\n6. Van Dyke Parks - As a composer, arranger, and songwriter, Parks contributed to baroque pop through his work with artists like Brian Wilson and The Beach Boys.\n\n7. Procol Harum - Their 1967 hit \"A Whiter Shade of Pale\" is considered a classic example of baroque pop, featuring organ and classical influences.\n\nThese artists and their works left a lasting impact on the baroque pop genre and its legacy.\n----------\n==================================================\n\nModel B:\n<<Model B's response to prompt 0>>: In 2012, Lana Del Rey's music was popular among a wide range of listeners, including fans of indie pop, alternative pop, and dream pop. She also gained a substantial following among younger listeners, particularly teenagers and people in their early twenties. Her music was also popular with critics and music industry insiders, and she received several award nominations in 2012. Additionally, her unique style and cinematic-inspired music videos helped her to gain a large online following.\n\nIt's worth noting that her album \"Born to Die\" was released in January 2012, which helped her to gain more popularity that year.\n\nShe also had a large number of followers on social media platforms such as Twitter and Instagram, where she would often share photos and updates with her fans.\n\nIn summary, in 2012, Lana Del Rey's music was listened to by a diverse group of people, including fans of indie pop, alternative pop, and dream pop, as well as younger listeners, critics, music industry insiders, and those who followed her on social media.\n----------\n<<Model B's response to prompt 1>>: Lana Del Rey, whose real name is Elizabeth Woolridge Grant, released her first extended play (EP) titled \"Kill Kill\" in October 2008 under the name \"Lizzy Grant\". However, she did not gain widespread recognition until the release of her debut studio album \"Born to Die\" in January 2012.\n\nIn 2010, Lana Del Rey was relatively unknown and her music was not widely available or promoted. It is likely that the people who were listening to her music in 2010 were mainly friends, family, and a small group of fans who had discovered her through her early EPs or live performances.\n\nIt is also possible that some music industry professionals, such as record label executives and A&R representatives, were listening to her music in 2010 as she was trying to secure a record deal.\n\nAdditionally, some music bloggers and critics who were keeping an eye on the up-and-coming artists may have been listening to her music in 2010.\n\nIn summary, it is likely that in 2010, Lana Del Rey's music was being listened to mainly by a small group of dedicated fans, friends, family, and music industry professionals, rather than the wider audience she would reach in 2012.\n----------\n<<Model B's response to prompt 2>>: In 2010, fans of indie pop, alternative pop, and dream pop were likely listening to a wide variety of artists and bands that were popular in those genres at the time. Here are a few examples of popular artists and bands in those genres in 2010:\n\n* Indie Pop: Arcade Fire, Sleigh Bells, Beach House, Best Coast, Tennis\n* Alternative Pop: The Black Keys, The National, Fleet Foxes, Mumford & Sons, Vampire Weekend\n* Dream Pop: Beach House, The XX, Wild Nothing, Washed Out, Memoryhouse\n\nIt's worth noting that the term \"indie pop\" and \"alternative pop\" are quite broad and encompasses a wide range of sounds, styles and bands, so it's hard to pinpoint exactly what people were listening to. Additionally, there were also many other artists and bands that were popular in 2010 that may not fall neatly into one of these genres but were still popular among fans of indie and alternative music.\n\nIt's also important to note that music consumption habits have changed a lot since 2010, with the rise of streaming platforms and social media, which have made it easier for fans to discover new music and for artists to reach a wider audience. So it's likely that in 2010, fans were listening to music through different means like radio, CDs, and digital downloads.\n----------\n<<Model B's response to prompt 3>>: Fashion tastes can vary widely among fans of indie pop, alternative pop, and dream pop, as these genres encompass a wide range of styles and subcultures. However, in 2010, some fashion trends that were popular among fans of these genres include:\n\n* Indie Pop: Vintage-inspired clothing, such as floral dresses, high-waisted shorts, and cardigans; band t-shirts; statement jewelry; and thick-rimmed glasses.\n* Alternative Pop: Skinny jeans, band t-shirts, leather jackets, boots, and plaid shirts.\n* Dream Pop: Oversized sweaters, floral dresses, high-waisted shorts, and ankle boots.\n\nIt's worth noting that these are general trends and that many fans of these genres may have had their own unique style, and may have been influenced by other subcultures and fashion movements. Additionally, fashion trends can change quickly and can vary widely from one region or city to another.\n\nIt's also important to note that music and fashion are closely related and many indie and alternative musicians often have their own unique style and fashion sense which their fans may emulate or be inspired by.\n\nIn summary, in 2010, among fans of indie pop, alternative pop, and dream pop, vintage-inspired clothing, band t-shirts, skinny jeans, leather jackets, boots, and plaid shirts were some of the fashion trends that were popular. But it's important to remember that fashion is a personal choice and many fans may have had their own unique style.\n----------\n<<Model B's response to prompt 4>>: Baroque pop, which is a style of pop music that incorporates elements of classical music, such as ornate orchestration, intricate arrangements and sophisticated harmonies, did exist in 2010. While it may not have been as widely popular or well-known as some other genres, there were certainly artists and bands that were creating and performing baroque pop music in 2010.\n\nSome examples of artists and bands that were making baroque pop music in 2010 include:\n\n* Sufjan Stevens: He is an American singer-songwriter, multi-instrumentalist, and record producer. His music is known for its intricate, orchestral arrangements and poetic lyrics.\n* Joanna Newsom: She is an American singer-songwriter, harpist, and actress. Her music is characterized by her use of the harp and her intricate, poetic lyrics.\n* Andrew Bird: He is an American singer-songwriter, multi-instrumentalist, and whistler. His music is known for its intricate, baroque-inspired arrangements and his use of violin and whistling.\n* Rufus Wainwright: He is a Canadian-American singer-songwriter known for his baroque pop-influenced music and his operatic tenor voice.\n\nIt's worth noting that Baroque pop is a genre that has been around for a while and it has had its peaks and valleys of popularity, in 2010 it might not have been as mainstream as it was in the late 60s and early 70s but it was still being created and performed by certain artists.\n\nIn summary, Baroque pop did exist in 2010, with artists such as Sufjan Stevens, Joanna Newsom, Andrew Bird and Rufus Wainwright making music that incorporated elements of classical music and ornate orchestration. While it may not have been as widely popular as some other genres, it was still being created and performed by certain artists.\n----------\n<<Model B's response to prompt 5>>: The late 1960s and early 1970s were a significant time for the development and popularization of baroque pop. During this period, many artists and bands were creating music that incorporated elements of classical music, such as ornate orchestration, intricate arrangements, and sophisticated harmonies. Here are a few examples of popular baroque pop artists and bands from the late 1960s and early 1970s:\n\n* The Beatles: The Beatles were one of the most influential bands in the history of popular music, and their music from the late 1960s, such as \"Sgt. Pepper's Lonely Hearts Club Band\" and \"Abbey Road,\" is often cited as an important influence on the development of baroque pop.\n* The Zombies: The Zombies were an English rock band formed in 1961 and disbanded in 1968. Their album \"Odessey and Oracle\" is considered a classic of the Baroque Pop genre.\n* The Moody Blues: The Moody Blues were an English rock band formed in 1964. They are known for their fusion of rock, classical, and folk music, and their use of orchestral arrangements.\n* Procol Harum: Procol Harum is an English rock band formed in 1967, known for their baroque pop sound characterized by sophisticated harmonies, intricate arrangements and ornate orchestration.\n* Scott Walker: Scott Walker was an American-born British singer-songwriter and producer who was active in the 1960s and 1970s and is known for his baroque pop sound characterized by sophisticated orchestrations, complex arrangements and dramatic delivery.\n\nIt's worth noting that this is not an exhaustive list, and there were many other artists and bands that were making baroque pop music during this period. Additionally, the term \"baroque pop\" was not widely used during this time, but the style was recognized and appreciated by many music critics and fans.\n\nIn summary, during the late 1960s and early 1970s, baroque pop was a popular genre, with artists such as The Beatles, The Zombies, The Moody Blues, Procol Harum and Scott Walker incorporating elements of classical music, ornate orchestration, intricate arrangements, and sophisticated harmonies in their music.\n----------\n","output_type":"stream"}]},{"cell_type":"code","source":"train.loc[:, 'token_count'] = get_token_lengths(train['text'])\n\n# prepare label for model\ntrain.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n\n# Display data\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:45:56.131136Z","iopub.execute_input":"2024-08-22T09:45:56.131586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.label.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# token Count\ndisplay(train['token_count'].describe().to_frame().astype(int))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get length of tokens which covers 90% of data, we'll still take 1024 length!\nnp.percentile(train['token_count'], 90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize Data\ntokens = tokenizer(\n    train['text'].tolist(), \n    padding='max_length', \n    max_length=CFG.MAX_LENGTH, \n    truncation=True, \n    return_tensors='np')\n\n# Input IDs are the token IDs\nINPUT_IDS = tokens['input_ids']\n# Attention Masks to Ignore Padding Tokens\nATTENTION_MASKS = tokens['attention_mask']\n# Label of Texts\nLABELS = train[['winner_model_a','winner_model_b','winner_tie']].values\n\nprint(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\nprint(f'LABELS shape: {LABELS.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_dataset(batch_size):\n    N_SAMPLES = LABELS.shape[0]\n    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n    while True:\n        # Shuffle Indices\n        np.random.shuffle(IDXS)\n        # Iterate Over All Indices Once\n        for idxs in IDXS.reshape(-1, batch_size):\n            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)\n            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)\n            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # Multi-label output\n            \n            # Shard Over TPU Nodes if applicable (you need to define mesh appropriately)\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n            \n            yield input_ids, attention_mask, labels\n\nTRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model for classification with 3 target label\nbase_model = LlamaForSequenceClassification.from_pretrained(\n    CFG.MODEL_NAME,\n    num_labels=CFG.NUM_LABELS,\n    torch_dtype=torch.bfloat16)\n\nbase_model.config.pretraining_tp = 1 \n\n# Assign Padding TOKEN\nbase_model.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=CFG.LORA_RANK,  # the dimension of the low-rank matrices\n    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations\n    lora_dropout= CFG.DROPOUT, \n    bias='none',\n    inference_mode=False,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=CFG.LORA_MODULES ) # Only Use Output and Values Projection","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create LoRa Model\nmodel = get_peft_model(base_model, lora_config)\nmodel.load_state_dict(torch.load(CFG.WEIGHTS_PATH), strict=False)\n\n# Trainable Parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of TPU Nodes\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n# distribute model\npartition_module(model, mesh)\n\nprint(f'num_devices: {num_devices}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verfy The Trainable Layers\nMODEL_LAYERS_ROWS = []\nTRAINABLE_PARAMS = []\nN_TRAINABLE_PARAMS = 0\n\nfor name, param in model.named_parameters():\n    # Layer Parameter Count\n    n_parameters = int(torch.prod(torch.tensor(param.shape)))\n    # Only Trainable Layers\n    if param.requires_grad:\n        # Add Layer Information\n        MODEL_LAYERS_ROWS.append({\n            'param': n_parameters,\n            'name': name,\n            'dtype': param.data.dtype,\n        })\n        # Append Trainable Parameter\n        TRAINABLE_PARAMS.append({ 'params': param })\n        # Add Number Of Trainable Parameters\"\n        N_TRAINABLE_PARAMS += n_parameters\n        \ndisplay(pd.DataFrame(MODEL_LAYERS_ROWS))\n\nprint(f\"\"\"\n===============================\nN_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}\nN_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}\n===============================\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LR & Optimizer\nN_SAMPLES = len(train)\nSTEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n\nOPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)\n\n# Cosine Learning Rate With Warmup\nlr_scheduler = transformers.get_cosine_schedule_with_warmup(\n    optimizer=OPTIMIZER,\n    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)\n\nprint(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the data type for the optimizer's state (e.g., momentum buffers)\nfor state in OPTIMIZER.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n            state[v] = v.to(dtype=torch.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids, attention_mask, labels = next(TRAIN_DATASET)\n\nprint(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')\nprint(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')\nprint(f'labels shape: {labels.shape}, dtype: {labels.dtype}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put Model In Train Mode\nmodel.train()\n\n# Loss Function, Cross Entropy\nLOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Begin to train model\")\nst = time()\nwarnings.filterwarnings(\"error\")\nMETRICS = {\n    'loss': [],\n    'accuracy': {'y_true': [], 'y_pred': [] }}\n\nfor epoch in tqdm(range(CFG.NUM_EPOCHS)):\n    ste = time()\n    for step in range(STEPS_PER_EPOCH):\n        # Zero Out Gradients\n        OPTIMIZER.zero_grad()\n        \n        # Get Batch\n        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n        \n        # Forward Pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n       \n        # Logits Float32\n        logits = outputs.logits.to(dtype=torch.float32)\n        \n        # Backward Pass\n        loss = LOSS_FN(logits,labels.to(dtype=torch.float32))\n        loss.backward()\n        \n        # optimizer step\n        OPTIMIZER.step()\n        xm.mark_step()\n        \n        # Update Learning Rate Scheduler\n        lr_scheduler.step()\n        \n        # Update Metrics And Progress Bar\n        METRICS['loss'].append(float(loss))\n        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()\n        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()\n        \n        if (step + 1) % 200 == 0:  \n            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n            metrics += ', µ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\\n                                                               METRICS['accuracy']['y_pred']))\n            lr = OPTIMIZER.param_groups[0]['lr']\n            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n            print(f'\\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')\n    \n    print(f'\\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )\n\n    # If stopped, and to continue training in future on tpu we save model and optimizer\n    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')\n    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')    \n    \n    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nplt.plot(METRICS['loss'])    \nplt.xlabel('Step per epoch')\nplt.ylabel('Loss')\nplt.title('Loss Plot step per epoch')    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.cpu()\ntorch.save(dict([(k,v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_full_1.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}